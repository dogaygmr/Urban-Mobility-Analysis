addmargins( table(filtered_data$car_count_hh) )
names(filtered_data)
colSums(is.na(filtered_data))
filtered_data$sexe <- factor(filtered_data$sexe, levels = c(1, 2), labels = c("male", "female"))
filtered_data$lien <- factor(filtered_data$lien, levels = c(1, 2, 3,4,5,6,7), labels = c("Reference Person", "Spouse","Child lives with reference","Child lives in 2 Home", "Grandparent","Uncle/Aunt","Housemate"))
filtered_data$mode_depl_ag <- factor(filtered_data$mode_depl_ag, levels = c("Autre", "MAP", "TCIU","TCU", "VP"), labels = c("Other", "by Walk","InterCity","BUS in City", "Car"))
filtered_data$permis_hh <- factor(filtered_data$permis_hh, levels = c(1, 2), labels = c("Yes", "No"))
selected_data <- filtered_data %>% select(-id_pers,-NO_PERS,-VP_DISPO,-id_depl)
names(selected_data)
#, and here are the variables in the final version:
#id_men", "sexe", "age", "permis", "lien", "mot_o_red", "jourdepl", "duree", "mode_depl_ag", "combo_id", "child_count_hh", "elderly_count_hh", "size_hh","permis_hh",  #"car_count_hh"
selected_data <- selected_data %>%
group_by(id_men) %>%
mutate(
Grandparent_HH = any(lien == "Grandparent"),
UncleAunt_HH = any(lien == "Uncle/Aunt"),
Housemate_HH = any(lien == "Housemate"),
Spouse_HH = any(lien == "Spouse")
) %>%
ungroup()
selected_data <- selected_data %>%
group_by(id_men) %>%
mutate(
Male_Count = sum(sexe == "male", na.rm = TRUE),
Female_Count = sum(sexe == "female", na.rm = TRUE)
) %>%
ungroup()
selected_data <- selected_data %>% select(-lien,-permis)
names(selected_data)
#this chunk is for controlling the factorisation
str(selected_data)
#for relevel
table(selected_data$mode_depl_ag)
selected_data <- selected_data %>%
filter(mode_depl_ag != "InterCity") %>%
mutate(mode_depl_ag = case_when(
mode_depl_ag %in% c("Other", "by Walk", "BUS in City") ~ "Others",
TRUE ~ mode_depl_ag
))
kable(table(selected_data$mode_depl_ag))
selected_data$mode_depl_ag <- as.factor(selected_data$mode_depl_ag)
selected_data$permis_hh <- as.factor(selected_data$permis_hh)
selected_data$Grandparent_HH <- as.factor(selected_data$Grandparent_HH)
selected_data$UncleAunt_HH <- as.factor(selected_data$UncleAunt_HH)
selected_data$Housemate_HH <- as.factor(selected_data$Housemate_HH)
selected_data$Spouse_HH <- as.factor(selected_data$Spouse_HH)
selected_data$jourdepl <- as.factor(selected_data$jourdepl)
#to check again
str(selected_data)
# relevel
selected_data$mode_depl_ag <- relevel(selected_data$mode_depl_ag,"Others")
sapply(selected_data, function(x) if(is.factor(x)) length(levels(x)) else NA)
selected_data <- selected_data %>% select(-Grandparent_HH)
names(selected_data)
#save(selected_data, file = "selected_data.RData")
#NULL MODEL:
model_simple <- glm(mode_depl_ag ~ 1, family = binomial, data = selected_data)
summary(model_simple)
# Fitting the binary logistic regression model
selected_data$mode_depl_ag <- as.factor(selected_data$mode_depl_ag)
selected_data$mode_depl_ag <- relevel(selected_data$mode_depl_ag, "Others")
model_full_binary <- glm(mode_depl_ag ~ sexe + age + mot_o_red +jourdepl + duree +child_count_hh + elderly_count_hh + size_hh + permis_hh + car_count_hh +UncleAunt_HH+ Housemate_HH +Spouse_HH+ Male_Count + Female_Count, family = binomial, data = selected_data)
#summary(model_full_binary)
significant_vars <- summary(model_full_binary)$coefficients[summary(model_full_binary)$coefficients[,4] <= 0.01, ]
# Extract only the Estimate and z value columns
significant_vars <- significant_vars[, c(1, 4)]
library(xtable)
latex_table <- xtable(significant_vars)
print(latex_table, type = 'latex', include.rownames = TRUE)
print("Null deviance: 6197.0 on 4530 degrees of freedom, Residual deviance: 4609.5 on 4510 degrees of freedom, AIC: 4651.5")
# Calculate variable importance
coef_summary <- summary(model_full_binary)$coefficients
variable_importance <- abs(coef_summary[, "Estimate"] / coef_summary[, "Std. Error"])
names(variable_importance) <- names(coef_summary[, "Estimate"])
# Split variable importance into two halves for plotting
halfway_point <- ceiling(length(variable_importance) / 2)
first_half <- variable_importance[1:halfway_point]
second_half <- variable_importance[(halfway_point + 1):length(variable_importance)]
# Increase the plotting area and split into two columns
par(mfrow=c(1, 1), mar=c(5, 8, 4, 2) + 0.1)
# Plot the first half
barplot(first_half, main="Variable Importance (1st Half)", horiz=TRUE, las=2, cex.names=0.6)
# Plot the second half
barplot(second_half, main="Variable Importance (2nd Half)", horiz=TRUE, las=1, cex.names=0.6)
# Reset to default par settings after plotting
par(mfrow=c(1, 1), mar=c(5, 4, 4, 2) + 0.1)
par(mfrow=c(2,2))
plot(model_full_binary)
# Identify potential outliers
influential <- cooks.distance(model_full_binary) > (4 / length(fitted(model_full_binary)))
plot(influential, type="h")
probabilities <- predict(model_full_binary, type = "response")
roc_obj <- roc(selected_data$mode_depl_ag, probabilities)
plot(roc_obj)
auc(roc_obj)
#The ROC curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1 - specificity) for a binary classifier system.
#AUC values range from 0 to 1, with a value of 0.5 suggesting no discrimination ability (equivalent to random chance), and a value of 1 indicating perfect separation between the classes.
# Calculate Cook's distances for the model(recalculating here, i know)
cooks_distances <- cooks.distance(model_full_binary)
threshold <- 4 / length(cooks_distances)
influential_indexes <- which(cooks_distances > threshold)
influential_data <- selected_data[influential_indexes, ]
print(influential_data)
# Descriptive statistics for the full dataset
full_stats <- selected_data %>% summarise(across(where(is.numeric), list(mean=mean, median=median, sd=sd)))
# Descriptive statistics for the influential points
influential_stats <- influential_data %>% summarise(across(where(is.numeric), list(mean=mean, median=median, sd=sd)))
print(full_stats)
print(influential_stats)
# Distribution analysis for a key variable (age)
full_age_distribution <- density(selected_data$age)
influential_age_distribution <- density(influential_data$age)
# Plot the distributions
plot(full_age_distribution, main="Age Distribution", xlab="Age", col="blue", xlim=range(c(selected_data$age, influential_data$age)))
lines(influential_age_distribution, col="red")
legend("topright", legend=c("Full Dataset", "Influential Points"), col=c("blue", "red"), lty=1)
# Group comparison for a categorical variable (mode_depl_ag)
full_group_counts <- selected_data %>% count(mode_depl_ag)
influential_group_counts <- influential_data %>% count(mode_depl_ag)
# Normalize counts by the number of records in each subset for comparison
full_group_counts$freq <- full_group_counts$n / nrow(selected_data)
influential_group_counts$freq <- influential_group_counts$n / nrow(influential_data)
# Merge for comparison
group_comparison <- merge(full_group_counts, influential_group_counts, by="mode_depl_ag", suffixes = c("_full", "_influential"))
# Print the comparison
print(group_comparison)
# For graphical comparison, let's create a boxplot of age for the full dataset and influential points
ggplot() +
geom_boxplot(data=selected_data, aes(x=factor(0), y=age), fill="blue", alpha=0.5) +
geom_boxplot(data=influential_data, aes(x=factor(1), y=age), fill="red", alpha=0.5) +
scale_x_discrete(labels=c("Full Dataset", "Influential Points")) +
labs(y="Age", x="Dataset", title="Boxplot of Age") +
theme_minimal()
# Fitting the binary logistic regression model
selected_data$mode_depl_ag <- as.factor(selected_data$mode_depl_ag)
selected_data$mode_depl_ag <- relevel(selected_data$mode_depl_ag, "Others")
model_reduced_binary <- glm(mode_depl_ag ~ sexe + age +child_count_hh+ elderly_count_hh +UncleAunt_HH+ Housemate_HH +Spouse_HH+ Male_Count + Female_Count, family = binomial, data = selected_data)
#summary(model_reduced_binary)
significant_vars1 <- summary(model_reduced_binary)$coefficients[summary(model_reduced_binary)$coefficients[,4] <= 0.01, ]
# Extract only the Estimate and z value columns
significant_vars1 <- significant_vars1[, c(1, 4)]
library(xtable)
latex_table1 <- xtable(significant_vars1)
print(latex_table1, type = 'latex', include.rownames = TRUE)
print("Null deviance: 6197.0 on 4530 degrees of freedom, Residual deviance: 5475.8 on 4521 degrees of freedom, AIC: 5495.8")
par(mfrow=c(2,2))
plot(model_reduced_binary)
probabilities <- predict(model_reduced_binary, type = "response")
roc_obj <- roc(selected_data$mode_depl_ag, probabilities)
plot(roc_obj)
auc(roc_obj)
library(boot)
set.seed(123) # for reproducibility
cv_full_model <- cv.glm(selected_data, model_full_binary, K = 10) # K-fold cross-validation
print(cv_full_model$delta)
library(rpart)
library(rpart.plot)
library(caret)
set.seed(123)
selected_variables <- c("sexe", "age", "mot_o_red", "jourdepl", "duree",
"child_count_hh", "elderly_count_hh", "size_hh",
"permis_hh", "car_count_hh", "UncleAunt_HH",
"Housemate_HH", "Spouse_HH", "Male_Count", "Female_Count",
"mode_depl_ag")
selected_data <- selected_data[, selected_variables]
# Split the data into training and test sets (70% training, 30% test)
splitIndex <- createDataPartition(selected_data$mode_depl_ag, p = 0.7,
list = FALSE,
times = 1)
training_data <- selected_data[splitIndex, ]
test_data <- selected_data[-splitIndex, ]
# Build a decision tree model using the training data
model_tree <- rpart(mode_depl_ag ~ ., data = training_data, method = "class")
# Visualize the decision tree
#prp(model_tree, type = 2, extra = 101)
# Make predictions on the test data
predictions <- predict(model_tree, newdata = test_data, type = "class")
# Print the confusion matrix
confusion_matrix <- table(predictions, test_data$mode_depl_ag)
kable((confusion_matrix))
# Calculate and print the accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
# Calculate precision, recall, and F1 score
cm <- confusionMatrix(as.factor(predictions), as.factor(test_data$mode_depl_ag))
print(cm$byClass)
duedate <- "31/01/2024"
## Libraries
library(tidyverse)
library(knitr)
library(ggplot2)
library(skimr)
library(dplyr)
library(readr)
library(broom)
library(nnet)
library(pROC)
#install.packages("Mongolite")
#library(mongolite)
## Global options
knitr::opts_chunk$set(               # Here we define behaviours of the code chunks in the Rmd
echo=T,        # to display code lines (False to hide them; but they will be ran)
eval=FALSE,     # to run the chunks
message=FALSE, # to hide in the document R messages while running chuncks
warning=FALSE, # to hide in the document R alerts
results='hide') #'hide' for hiding outputs, 'asis' for display in generated pdf document
knitr::opts_knit$set(width=75)
options(max.print="200")       # to define max number of elements (lines) displayed by R when running an instruction
load("allgre.PB_V2.RData")
#names(allgre.PB_V2)
#nrow(allgre.PB_V2)
# Creation of new id_dep1
sum(is.na(allgre.PB_V2$id_depl))
allgre.PB_V2$id_dep1 <- ifelse(
!is.na(allgre.PB_V2$id_pers) & !is.na(allgre.PB_V2$NO_DEPL) & !is.na(allgre.PB_V2$mode_depl_ag),
allgre.PB_V2$id_pers * 10 + allgre.PB_V2$NO_DEPL,
NA  # Koşul sağlanmazsa NA atar
)
table(is.na(allgre.PB_V2$id_depl))
table(allgre.PB_V2$nbd)
sum(is.na(allgre.PB_V2$id_depl))
#By determining whether the values in the nbd column are different from zero, it keeps this information as (true/false) in the column named UN
allgre.PB_V2$UN <- as.numeric(allgre.PB_V2$nbd !=0)
table(as.numeric(allgre.PB_V2$UN))
sum(is.na(allgre.PB_V2$mode_depl_ag))
# Check if UN column is FALSE (0) for NA values in mode_depl_ag column
UN_is_FALSE_for_mode_depl_ag_NA <- with(allgre.PB_V2, is.na(mode_depl_ag) & UN == 0)
table(UN_is_FALSE_for_mode_depl_ag_NA)
#drop 1277 record which are immobile and has nan values in mode_depl_ag
sum(is.na(allgre.PB_V2$mode_depl_ag))
sum(is.na(allgre.PB_V2$id_depl))
allgre.PB_V2 <- allgre.PB_V2[!is.na(allgre.PB_V2$mode_depl_ag), ]
sum(is.na(allgre.PB_V2$mode_depl_ag))
sum(is.na(allgre.PB_V2$id_depl))
allgre.PB_V2 <- allgre.PB_V2 %>% mutate(
mot_o_red = factor( case_when(
motifor %in% c( 1, 2)	~ 1, #  domicile & residence secondaire
motifor %in% c(11, 12) 	~ 2, 	#	/*travail*/
motifor %in% c(21, 22, 26)	~ 3, 	#	/*primaire*/
motifor %in% c(23, 24, 27, 28)  ~ 4 ,	#	/*col-lyc*/
motifor %in% c( 25, 29)		~ 5,  # /*univ*/
30 < motifor & motifor < 44	~ 6,	#	/*achat serv*/
49 < motifor & motifor < 55	~ 7,	#	/*loisir*/
60 < motifor & motifor < 75	~ 8,	#	/*acct*/
motifor == 91 | motifor == NA	~ 9, #	/*autre*/
motifor == 81 		~ 10	,  #/*tournee prof*/
(motifdes == 91 | motifdes == NA	) & nbd==0 ~ 0
# else 0
))
)
motif.red <- c("DOMICILE" , "TRAVAIL" , "ECOLE PRIMAIRE" , "C.E.S.-LYCEE" ,"UNIVERSITE" ,'ACHAT' ,'LOISIR' ,'ACCOMPAGNEMENT' , "AUTRES", "Professionnel")
allgre.PB_V2$mot_o_red <- factor(allgre.PB_V2$mot_o_red,  labels =  motif.red)
table(allgre.PB_V2$mot_o_red )
length(unique(allgre.PB_V2$id_pers))
reduced1 <- distinct(allgre.PB_V2, id_depl, .keep_all = T)
colSums(is.na(reduced1))
table(reduced1$mot_o_red)
nrow(reduced1)
length(unique(reduced1$id_pers))
selected_data = subset(reduced1, select = c(id_pers,id_men, NO_PERS, sexe, age,permis,VP_DISPO,lien,id_depl,mot_o_red, jourdepl,duree, mode_depl_ag ))
colSums(is.na(selected_data))
nrow(selected_data)
length(unique(selected_data$id_pers))
selected_data$mot_o_red <- factor(selected_data$mot_o_red,  labels =  motif.red)
table(selected_data$mot_o_red )
filtered_data <- selected_data %>%
filter(mot_o_red %in% levels(mot_o_red)[2:5])
nrow(filtered_data)
length(unique(filtered_data$id_pers))
addmargins( table(filtered_data$duree))
table(is.na(filtered_data$duree))
table(cut(filtered_data$duree, breaks=seq(0, max(filtered_data$duree, na.rm = T), 30)), useNA = "always")
# Select the row with the highest duree value for each unique id_pers
filtered_data<- filtered_data %>%
group_by(id_pers) %>%
filter(duree == max(duree, na.rm = TRUE)) %>%
ungroup()
nrow(filtered_data)
length(unique(filtered_data$id_pers))
analysis <- filtered_data %>%
group_by(id_pers, duree) %>% # Group by both id_pers and duree
summarise(mode_depl_ag_values = n_distinct(mode_depl_ag), .groups = 'drop') %>%
filter(mode_depl_ag_values > 1) # Filter groups where mode_depl_ag has more than one distinct value
print(analysis)
filtered_data <- filtered_data %>%
arrange(id_pers, duree, mode_depl_ag) %>%
group_by(id_pers, duree) %>%
mutate(
combo_id = paste(id_pers, duree, rank(mode_depl_ag, ties.method = "first"), sep = "_")
) %>%
ungroup()
#kable(filtered_data)
nrow(filtered_data)
length(unique(filtered_data$combo_id))
table(filtered_data$permis)
filtered_data <- filtered_data %>%
mutate(permis = case_when(
permis %in% c(1, 3) ~ 1,
TRUE ~ permis
))
table(filtered_data$permis)
#child count for each hh
Is_Child <- filtered_data$age < 18
addmargins( table(Is_Child))
filtered_data$child_count_hh <-ave(Is_Child, filtered_data$id_men, FUN=sum)
addmargins( table(filtered_data$child_count_hh))
print('         ')
#living with elderly people check, count old people per hh
Is_Old <- filtered_data$age > 65
addmargins( table(Is_Old))
filtered_data$elderly_count_hh <-ave(Is_Old, filtered_data$id_men, FUN=sum)
addmargins( table(filtered_data$elderly_count_hh))
#size of hh
print('            ')
print("size of hh")
filtered_data$size_hh <- ave(filtered_data$NO_PERS, filtered_data$id_men, FUN = max)
addmargins( table(filtered_data$size_hh) )
#permis posession by hh
print('         ')
print("permis posession by hh")
filtered_data$permis_hh <- ave(filtered_data$permis, filtered_data$id_men, FUN = min)
addmargins(table(filtered_data$permis_hh))
#car count per hh
print('          ')
print("car count per hh")
filtered_data$car_count_hh <- ave(filtered_data$VP_DISPO, filtered_data$id_men, FUN = max)
addmargins( table(filtered_data$car_count_hh) )
names(filtered_data)
colSums(is.na(filtered_data))
filtered_data$sexe <- factor(filtered_data$sexe, levels = c(1, 2), labels = c("male", "female"))
filtered_data$lien <- factor(filtered_data$lien, levels = c(1, 2, 3,4,5,6,7), labels = c("Reference Person", "Spouse","Child lives with reference","Child lives in 2 Home", "Grandparent","Uncle/Aunt","Housemate"))
filtered_data$mode_depl_ag <- factor(filtered_data$mode_depl_ag, levels = c("Autre", "MAP", "TCIU","TCU", "VP"), labels = c("Other", "by Walk","InterCity","BUS in City", "Car"))
filtered_data$permis_hh <- factor(filtered_data$permis_hh, levels = c(1, 2), labels = c("Yes", "No"))
selected_data <- filtered_data %>% select(-id_pers,-NO_PERS,-VP_DISPO,-id_depl)
names(selected_data)
#, and here are the variables in the final version:
#id_men", "sexe", "age", "permis", "lien", "mot_o_red", "jourdepl", "duree", "mode_depl_ag", "combo_id", "child_count_hh", "elderly_count_hh", "size_hh","permis_hh",  #"car_count_hh"
selected_data <- selected_data %>%
group_by(id_men) %>%
mutate(
Grandparent_HH = any(lien == "Grandparent"),
UncleAunt_HH = any(lien == "Uncle/Aunt"),
Housemate_HH = any(lien == "Housemate"),
Spouse_HH = any(lien == "Spouse")
) %>%
ungroup()
selected_data <- selected_data %>%
group_by(id_men) %>%
mutate(
Male_Count = sum(sexe == "male", na.rm = TRUE),
Female_Count = sum(sexe == "female", na.rm = TRUE)
) %>%
ungroup()
selected_data <- selected_data %>% select(-lien,-permis)
names(selected_data)
#this chunk is for controlling the factorisation
str(selected_data)
#for relevel
table(selected_data$mode_depl_ag)
selected_data <- selected_data %>%
filter(mode_depl_ag != "InterCity") %>%
mutate(mode_depl_ag = case_when(
mode_depl_ag %in% c("Other", "by Walk", "BUS in City") ~ "Others",
TRUE ~ mode_depl_ag
))
kable(table(selected_data$mode_depl_ag))
selected_data$mode_depl_ag <- as.factor(selected_data$mode_depl_ag)
selected_data$permis_hh <- as.factor(selected_data$permis_hh)
selected_data$Grandparent_HH <- as.factor(selected_data$Grandparent_HH)
selected_data$UncleAunt_HH <- as.factor(selected_data$UncleAunt_HH)
selected_data$Housemate_HH <- as.factor(selected_data$Housemate_HH)
selected_data$Spouse_HH <- as.factor(selected_data$Spouse_HH)
selected_data$jourdepl <- as.factor(selected_data$jourdepl)
#to check again
str(selected_data)
# relevel
selected_data$mode_depl_ag <- relevel(selected_data$mode_depl_ag,"Others")
sapply(selected_data, function(x) if(is.factor(x)) length(levels(x)) else NA)
selected_data <- selected_data %>% select(-Grandparent_HH)
names(selected_data)
#save(selected_data, file = "selected_data.RData")
#NULL MODEL:
model_simple <- glm(mode_depl_ag ~ 1, family = binomial, data = selected_data)
summary(model_simple)
# Fitting the binary logistic regression model
selected_data$mode_depl_ag <- as.factor(selected_data$mode_depl_ag)
selected_data$mode_depl_ag <- relevel(selected_data$mode_depl_ag, "Others")
model_full_binary <- glm(mode_depl_ag ~ sexe + age + mot_o_red +jourdepl + duree +child_count_hh + elderly_count_hh + size_hh + permis_hh + car_count_hh +UncleAunt_HH+ Housemate_HH +Spouse_HH+ Male_Count + Female_Count, family = binomial, data = selected_data)
#summary(model_full_binary)
significant_vars <- summary(model_full_binary)$coefficients[summary(model_full_binary)$coefficients[,4] <= 0.01, ]
# Extract only the Estimate and z value columns
significant_vars <- significant_vars[, c(1, 4)]
library(xtable)
latex_table <- xtable(significant_vars)
print(latex_table, type = 'latex', include.rownames = TRUE)
print("Null deviance: 6197.0 on 4530 degrees of freedom, Residual deviance: 4609.5 on 4510 degrees of freedom, AIC: 4651.5")
# Calculate variable importance
coef_summary <- summary(model_full_binary)$coefficients
variable_importance <- abs(coef_summary[, "Estimate"] / coef_summary[, "Std. Error"])
names(variable_importance) <- names(coef_summary[, "Estimate"])
# Split variable importance into two halves for plotting
halfway_point <- ceiling(length(variable_importance) / 2)
first_half <- variable_importance[1:halfway_point]
second_half <- variable_importance[(halfway_point + 1):length(variable_importance)]
# Increase the plotting area and split into two columns
par(mfrow=c(1, 1), mar=c(5, 8, 4, 2) + 0.1)
# Plot the first half
barplot(first_half, main="Variable Importance (1st Half)", horiz=TRUE, las=2, cex.names=0.6)
# Plot the second half
barplot(second_half, main="Variable Importance (2nd Half)", horiz=TRUE, las=1, cex.names=0.6)
# Reset to default par settings after plotting
par(mfrow=c(1, 1), mar=c(5, 4, 4, 2) + 0.1)
par(mfrow=c(2,2))
plot(model_full_binary)
# Identify potential outliers
influential <- cooks.distance(model_full_binary) > (4 / length(fitted(model_full_binary)))
plot(influential, type="h")
probabilities <- predict(model_full_binary, type = "response")
roc_obj <- roc(selected_data$mode_depl_ag, probabilities)
plot(roc_obj)
auc(roc_obj)
#The ROC curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1 - specificity) for a binary classifier system.
#AUC values range from 0 to 1, with a value of 0.5 suggesting no discrimination ability (equivalent to random chance), and a value of 1 indicating perfect separation between the classes.
# Calculate Cook's distances for the model(recalculating here, i know)
cooks_distances <- cooks.distance(model_full_binary)
threshold <- 4 / length(cooks_distances)
influential_indexes <- which(cooks_distances > threshold)
influential_data <- selected_data[influential_indexes, ]
print(influential_data)
# Descriptive statistics for the full dataset
full_stats <- selected_data %>% summarise(across(where(is.numeric), list(mean=mean, median=median, sd=sd)))
# Descriptive statistics for the influential points
influential_stats <- influential_data %>% summarise(across(where(is.numeric), list(mean=mean, median=median, sd=sd)))
print(full_stats)
print(influential_stats)
# Distribution analysis for a key variable (age)
full_age_distribution <- density(selected_data$age)
influential_age_distribution <- density(influential_data$age)
# Plot the distributions
plot(full_age_distribution, main="Age Distribution", xlab="Age", col="blue", xlim=range(c(selected_data$age, influential_data$age)))
lines(influential_age_distribution, col="red")
legend("topright", legend=c("Full Dataset", "Influential Points"), col=c("blue", "red"), lty=1)
# Group comparison for a categorical variable (mode_depl_ag)
full_group_counts <- selected_data %>% count(mode_depl_ag)
influential_group_counts <- influential_data %>% count(mode_depl_ag)
# Normalize counts by the number of records in each subset for comparison
full_group_counts$freq <- full_group_counts$n / nrow(selected_data)
influential_group_counts$freq <- influential_group_counts$n / nrow(influential_data)
# Merge for comparison
group_comparison <- merge(full_group_counts, influential_group_counts, by="mode_depl_ag", suffixes = c("_full", "_influential"))
# Print the comparison
print(group_comparison)
# For graphical comparison, let's create a boxplot of age for the full dataset and influential points
ggplot() +
geom_boxplot(data=selected_data, aes(x=factor(0), y=age), fill="blue", alpha=0.5) +
geom_boxplot(data=influential_data, aes(x=factor(1), y=age), fill="red", alpha=0.5) +
scale_x_discrete(labels=c("Full Dataset", "Influential Points")) +
labs(y="Age", x="Dataset", title="Boxplot of Age") +
theme_minimal()
# Fitting the binary logistic regression model
selected_data$mode_depl_ag <- as.factor(selected_data$mode_depl_ag)
selected_data$mode_depl_ag <- relevel(selected_data$mode_depl_ag, "Others")
model_reduced_binary <- glm(mode_depl_ag ~ sexe + age +child_count_hh+ elderly_count_hh +UncleAunt_HH+ Housemate_HH +Spouse_HH+ Male_Count + Female_Count, family = binomial, data = selected_data)
#summary(model_reduced_binary)
significant_vars1 <- summary(model_reduced_binary)$coefficients[summary(model_reduced_binary)$coefficients[,4] <= 0.01, ]
# Extract only the Estimate and z value columns
significant_vars1 <- significant_vars1[, c(1, 4)]
library(xtable)
latex_table1 <- xtable(significant_vars1)
print(latex_table1, type = 'latex', include.rownames = TRUE)
print("Null deviance: 6197.0 on 4530 degrees of freedom, Residual deviance: 5475.8 on 4521 degrees of freedom, AIC: 5495.8")
par(mfrow=c(2,2))
plot(model_reduced_binary)
probabilities <- predict(model_reduced_binary, type = "response")
roc_obj <- roc(selected_data$mode_depl_ag, probabilities)
plot(roc_obj)
auc(roc_obj)
library(boot)
set.seed(123) # for reproducibility
cv_full_model <- cv.glm(selected_data, model_full_binary, K = 10) # K-fold cross-validation
print(cv_full_model$delta)
library(rpart)
library(rpart.plot)
library(caret)
set.seed(123)
selected_variables <- c("sexe", "age", "mot_o_red", "jourdepl", "duree",
"child_count_hh", "elderly_count_hh", "size_hh",
"permis_hh", "car_count_hh", "UncleAunt_HH",
"Housemate_HH", "Spouse_HH", "Male_Count", "Female_Count",
"mode_depl_ag")
selected_data <- selected_data[, selected_variables]
# Split the data into training and test sets (70% training, 30% test)
splitIndex <- createDataPartition(selected_data$mode_depl_ag, p = 0.7,
list = FALSE,
times = 1)
training_data <- selected_data[splitIndex, ]
test_data <- selected_data[-splitIndex, ]
# Build a decision tree model using the training data
model_tree <- rpart(mode_depl_ag ~ ., data = training_data, method = "class")
# Visualize the decision tree
#prp(model_tree, type = 2, extra = 101)
# Make predictions on the test data
predictions <- predict(model_tree, newdata = test_data, type = "class")
# Print the confusion matrix
confusion_matrix <- table(predictions, test_data$mode_depl_ag)
kable((confusion_matrix))
# Calculate and print the accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")
# Calculate precision, recall, and F1 score
cm <- confusionMatrix(as.factor(predictions), as.factor(test_data$mode_depl_ag))
print(cm$byClass)
